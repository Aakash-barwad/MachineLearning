{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indego Bike Duration Prediction Using Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very First step ( optional ) , as data was not in proper nomenclature, so convert it into same nomenclature and collate in a single folder for easier access. In our case, 'data' folder contain all the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Importing important packages required for operation..\n",
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.base import TransformerMixin,BaseEstimator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder,Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all data files from the specific folder path or could be from remote machine path. Glob function is used to get all files list from directory, assuming all files is in .csv format for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data load function is used for loading data at a given directory or path.\n",
    "\n",
    "@param :\n",
    "       path : path could be directory of local or remote folder.\n",
    "       \n",
    "@return :\n",
    "      \n",
    "      df - data frame concatenated of all the files together in row wise ( axis = 0 )\n",
    "\n",
    "'''\n",
    "def data_load(path):\n",
    "    \n",
    "    files = glob.glob(\"%s\\*.csv\"%(path))\n",
    "    \n",
    "    for i,file in enumerate(files):\n",
    "        if i == 0 :\n",
    "            df = pd.read_csv(file)\n",
    "        else:\n",
    "            df = pd.concat([df,pd.read_csv(file)],axis=0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "#load data....\n",
    "df = data_load('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('all_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Indego(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit( self, df, y = None ):\n",
    "        print(\"in indego fit\")\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df, y = None):\n",
    "    \n",
    "        def time_convert(df,time_col):\n",
    "            df[time_col] = pd.to_datetime(arg=df[time_col], infer_datetime_format=True)\n",
    "            return df\n",
    "    \n",
    "        def drop_column(df, drop_col_name):\n",
    "            df.drop(columns=[drop_col_name],axis=1,inplace=True)\n",
    "            return df\n",
    "        \n",
    "        def punctuation_cleaning(df,punc_col):\n",
    "            char_index = df[df[punc_col].str.contains('[A-Za-z]', na=False)].index\n",
    "            df.drop(char_index,inplace=True)\n",
    "            return df\n",
    "        \n",
    "        def remove_null(df,col_name):\n",
    "            df = df.loc[~df[col_name].isnull()]\n",
    "            return df\n",
    "        \n",
    "        def convert_data_type(df, col_list,data_type):\n",
    "            df[col_list] = df[col_list].astype(data_type)\n",
    "            return df\n",
    "    \n",
    "        def station_cleaning(df, station_col_list,final_station_col):\n",
    "            drop_index = df.loc[(df[station_col_list[0]].isnull()) & (df[station_col_list[1]].isnull())].index\n",
    "            df.drop(drop_index,inplace=True)\n",
    "            df[station_col_list] = df[station_col_list].fillna('')\n",
    "            df[final_station_col] = df[station_col_list[0]].astype(str) +df[station_col_list[1]].astype(str)\n",
    "            df.drop(columns=station_col_list,inplace=True)\n",
    "            return df\n",
    "    \n",
    "        def lat_lon_cleaning(df,lat_lon_col):\n",
    "            lat_lon_null = df.loc[(df[lat_lon_col[0]].isnull()) & (df[lat_lon_col[1]].isnull())].index\n",
    "            df.drop(lat_lon_null,inplace=True)\n",
    "            df =  remove_null(df,lat_lon_col[1])\n",
    "            df = remove_null(df,lat_lon_col[0])\n",
    "            return df\n",
    "    \n",
    "        def remove_lat_lon_outlier(df,lat_lon_list):\n",
    "            df = df.loc[(df[lat_lon_list[0]]!=0) | (df[lat_lon_list[1]]!=0)]\n",
    "            df = punctuation_cleaning(df,lat_lon_list[0])\n",
    "            return df\n",
    "    \n",
    "        def change_lang_lat_value(df):\n",
    "            df.loc[df.start_lat <=0,'start_lat'] = abs(df.start_lat)\n",
    "            df.loc[df.end_lat <=0,'end_lat'] = abs(df.end_lat)\n",
    "            return df\n",
    "    \n",
    "        def degree_to_radion(degree):\n",
    "            return degree*(np.pi/180)\n",
    "\n",
    "        def calculate_distance(pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude):\n",
    "\n",
    "            from_lat = degree_to_radion(pickup_latitude)\n",
    "            from_long = degree_to_radion(pickup_longitude)\n",
    "            to_lat = degree_to_radion(dropoff_latitude)\n",
    "            to_long = degree_to_radion(dropoff_longitude)\n",
    "\n",
    "            radius = 6371.01\n",
    "\n",
    "            lat_diff = to_lat - from_lat\n",
    "            long_diff = to_long - from_long\n",
    "\n",
    "            a = np.sin(lat_diff / 2)**2 + np.cos(degree_to_radion(from_lat)) * np.cos(degree_to_radion(to_lat)) * np.sin(long_diff / 2)**2\n",
    "            c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "\n",
    "            return radius * c\n",
    "    \n",
    "        def add_new_date_time_features(dataset):\n",
    "            dataset['hour'] = dataset.start_time.dt.hour\n",
    "            dataset['day'] = dataset.start_time.dt.day\n",
    "            dataset['month'] = dataset.start_time.dt.month\n",
    "            dataset['year'] = dataset.start_time.dt.year\n",
    "            dataset['day_of_week'] = dataset.start_time.dt.dayofweek\n",
    "\n",
    "            return dataset\n",
    "       \n",
    "        print(\"in transformer method\")\n",
    "        #df = time_convert(df,'start_time')\n",
    "        df.set_index('trip_id',inplace=True)\n",
    "        df = station_cleaning(df,['start_station','start_station_id'],'start_station_complete')\n",
    "        df = station_cleaning(df,['end_station','end_station_id'],'end_station_complete')\n",
    "        df = punctuation_cleaning(df,'bike_id')\n",
    "        df = remove_null(df,'bike_id')\n",
    "        df = lat_lon_cleaning(df,['end_lat','start_lat'])\n",
    "        df = remove_lat_lon_outlier(df,['end_lat','end_lon'])\n",
    "        df = remove_lat_lon_outlier(df,['start_lat','start_lon'])\n",
    "        df = convert_data_type(df,['start_lat','start_lon','end_lat','end_lon'],float)\n",
    "        df = change_lang_lat_value(df)\n",
    "        df['distance'] = calculate_distance(df.start_lat, df.start_lon, df.end_lat, df.end_lon)\n",
    "        #df = add_new_date_time_features(df)\n",
    "        df = convert_data_type(df,['start_station_complete','end_station_complete','bike_id'],float)\n",
    "        drop_list = ['bike_type','passholder_type','start_time','end_time','end_lat','end_lon','start_lat','start_lon']\n",
    "        df.drop(columns = drop_list,axis=1,inplace=True)        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_label_encoder(BaseEstimator,TransformerMixin):\n",
    "    def fit( self, df, y = None ):\n",
    "        print(\"in label encoder fit.....\")\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df, y = None):\n",
    "        print(\"in label encoder transformer\")\n",
    "        label = LabelEncoder()\n",
    "        df['trip_route'] = label.fit_transform(df['trip_route_category'])\n",
    "        df.drop(columns='trip_route_category',inplace=True)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obj = Indego().fit(df).transform(df)\n",
    "pre_pipeline = Pipeline(steps=[('preprocess',Indego()),\n",
    "                               ('label_encoder',custom_label_encoder()),\n",
    "                               ('Normalizer',Normalizer()),\n",
    "                               ('decision tree', DecisionTreeRegressor())\n",
    "                              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pre_pipeline.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_build():\n",
    "    preprocess_pipeline = Pipeline(steps=[('preprocess',Indego()),('label',custom_label_encoder())])\n",
    "    dataset = preprocess_pipeline.fit_transform(df) \n",
    "                                   \n",
    "    X = dataset.drop(columns=['duration'])\n",
    "    y = dataset['duration']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.20,random_state=42)\n",
    "    \n",
    "    y_train = y_train /60\n",
    "    y_test =  y_test / 60\n",
    "    \n",
    "    y_train = np.log1p(y_train)\n",
    "    y_test = np.log1p(y_test)\n",
    "#     print(X_train.shape,y_train.shape)\n",
    "#     print(y_test)\n",
    "    model = Pipeline(steps=[('Normalizer',Normalizer()),\n",
    "                               ('decision tree', DecisionTreeRegressor())\n",
    "                            ])\n",
    "    model.fit(X_train,y_train)\n",
    "#     pred = model.predict(X_test)\n",
    "#     print(np.expm1(y_test))\n",
    "#     print(np.expm1(model.predict(X_test)))\n",
    "#     print(np.sqrt(mean_squared_error(np.expm1(y_test),np.expm1(model.predict(X_test)))))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in indego fit\n",
      "in transformer method\n",
      "in label encoder fit.....\n",
      "in label encoder transformer\n"
     ]
    }
   ],
   "source": [
    "model = model_build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'finalized_model.pkl'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_pipeline = Pipeline(steps=[('preprocess',Indego()),('label',custom_label_encoder())])\n",
    "dataset = preprocess_pipeline.fit_transform(df)\n",
    "X = dataset.drop(columns=['duration'])\n",
    "y = dataset['duration']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.20,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b= Normalizer().fit(X_train,y_train).transform(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Convert string type column into datetime format so we can use further its attribute.\n",
    "\n",
    "@param:\n",
    "    time_col : name of column which contain datetime value.\n",
    "\n",
    "@return:\n",
    "    df : dataframe which contain date time column.\n",
    "\n",
    "'''\n",
    "def time_convert(df,time_col):\n",
    "    df[time_col] = pd.to_datetime(arg=df[time_col], infer_datetime_format=True)\n",
    "    return df\n",
    "\n",
    "df = time_convert(df,'start_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set trip id as index which is unique as well as complete.\n",
    "df.set_index('trip_id',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.copy()\n",
    "df.shape\n",
    "df.info()\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['start_time'] = pd.to_datetime(arg=df['start_time'], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorting based on start time so we can check time wise trend and changes..\n",
    "df.sort_values('start_time',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop bike type as almost more than 50% record are null and other 50% have biased towards specific value.. \n",
    "df.drop(columns=['bike_type'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Drop a column from dataframe and return new dataframe without deleted column.\n",
    "\n",
    "@param:\n",
    "    df : dataframe contain all column including which need to be deleted.\n",
    "    drop_col_name : name of the column which need to be dropped.\n",
    "    \n",
    "@return:\n",
    "\n",
    "    df : a dataframe all column except deleted column.\n",
    "    \n",
    "'''\n",
    "def drop_column(df, drop_col_name):\n",
    "    df.drop(columns=[drop_col_name],inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we station column, first we see how much null values in each columns ( taking start_station & start_station_id ). I figured out both have contradictory in term of data. i.e. if station has value than station_id doesn't have and vice versa. that means this column correspond to same value so try to reduce in 1 column.\n",
    "\n",
    "first thing to remove those rows in which both of these column contain null values.\n",
    "later we fill blanks because we need to concatenate togther this column to get a complete new column without change of meaning.\n",
    "\n",
    "converted into str type to match type and added together and finally achieved a complete column, remove these two column as our new column is decribing two columns.\n",
    "\n",
    "same operation for end station as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Station cleaning is used to clean station and station_id.\n",
    "\n",
    "@param :\n",
    "      df : a dataframe contain all the colunmns\n",
    "      station_col_list: It contain start_station and start_station_id similarly for end station as well.\n",
    "      \n",
    "@return :\n",
    "      df : return a dataframe\n",
    "\n",
    "'''\n",
    "def station_cleaning(df, station_col_list,final_station_col):\n",
    "    drop_index = df.loc[(df[station_col_list[0]].isnull()) & (df[station_col_list[1]].isnull())].index\n",
    "    df.drop(drop_index,inplace=True)\n",
    "    df[station_col_list] = df[station_col_list].fillna('')\n",
    "    df[final_station_col] = df[station_col_list[0]].astype(str) +df[station_col_list[1]].astype(str)\n",
    "    df.drop(columns=station_col_list,inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = station_cleaning(df,['start_station','start_station_id'],'start_station_complete')\n",
    "df = station_cleaning(df,['end_station','end_station_id'],'end_station_complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_index = df.loc[(df.start_station.isnull()) & (df.start_station_id.isnull())].index\n",
    "# df.drop(drop_index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc[(~df.start_station.isnull()) & (df.start_station_id.isnull())].shape\n",
    "# df.loc[(df.start_station.isnull()) & (~df.start_station_id.isnull())].shape\n",
    "# df[['start_station','start_station_id']] = df[['start_station','start_station_id']].fillna('')\n",
    "# df['start_station_complete'] = df['start_station'].astype(str) +df['start_station_id'].astype(str)\n",
    "# df.drop(columns=['start_station','start_station_id'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc[(~df.end_station.isnull()) & (df.end_station_id.isnull())].shape\n",
    "# df.loc[(df.end_station.isnull()) & (~df.end_station_id.isnull())].shape\n",
    "# df[['end_station','end_station_id']] = df[['end_station','end_station_id']].fillna('')\n",
    "# df['end_station_complete'] = df['end_station'].astype(str) +df['end_station_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop(columns=['end_station','end_station_id'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Punctation cleaning is required because dataset metadata shows its integer column and as well approx 99% value are float/int only. secondly few rows have value like 'delete me' , it could be anything in future so for safer side remove all alpha value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Punctuation cleaning is used to clean any character present in integer or float column for now removing alphabets.\n",
    "\n",
    "@param :\n",
    "      df : a dataframe contain all the columns.\n",
    "      punc_col : a column contain special or alphabet character.\n",
    "      \n",
    "@return :\n",
    "      \n",
    "      df : a dataframe with removed puctutation from specific column.\n",
    "      \n",
    "'''\n",
    "\n",
    "def punctuation_cleaning(df,punc_col):\n",
    "    char_index = df[df[punc_col].str.contains('[A-Za-z]', na=False)].index\n",
    "    df.drop(char_index,inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Remove null values from a specific column.\n",
    "\n",
    "@param :\n",
    "        df - a dataframe which contain all columns\n",
    "        col_name - name of column which contain null values.\n",
    "        \n",
    "@return :\n",
    "       df - a dataframe with removed null values from specific column.\n",
    "\n",
    "'''\n",
    "\n",
    "def remove_null(df,col_name):\n",
    "    df = df.loc[~df[col_name].isnull()]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df['bike_id'].str.contains('[A-Za-z]',na=False)]\n",
    "# df = df.loc[df['bike_id'] !='delete me']\n",
    "# char_index = df[df['bike_id'].str.contains('[A-Za-z]', na=False)].index\n",
    "# df.drop(char_index,inplace=True)\n",
    "# df = df.loc[~df.bike_id.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = punctuation_cleaning(df,'bike_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = remove_null(df,'bike_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lat_lon_cleaning(df,lat_lon_col):\n",
    "    lat_lon_null = df.loc[(df[lat_lon_col[0]].isnull()) & (df[lat_lon_col[1]].isnull())].index\n",
    "    df.drop(lat_lon_null,inplace=True)\n",
    "    df = remove_null(df,lat_lon_col[1])\n",
    "    df = remove_null(df,lat_lon_col[0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = lat_lon_cleaning(df,['end_lat','start_lat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_lat_lon_outlier(df,lat_lon_list):\n",
    "    df = df.loc[(df[lat_lon_list[0]]!=0) | (df[lat_lon_list[1]]!=0)]\n",
    "    df = punctuation_cleaning(df,lat_lon_list[0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = remove_lat_lon_outlier(df,['end_lat','end_lon'])\n",
    "df = remove_lat_lon_outlier(df,['start_lat','start_lon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_type(df, col_list,data_type):\n",
    "    df[col_list] = df[col_list].astype(data_type)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = convert_data_type(df,['start_lat','start_lon','end_lat','end_lon'],float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_lang_lat_value(df):\n",
    "    df.loc[df.start_lat <=0,'start_lat'] = abs(df.start_lat)\n",
    "    df.loc[df.end_lat <=0,'end_lat'] = abs(df.end_lat)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = change_lang_lat_value(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degree_to_radion(degree):\n",
    "    return degree*(np.pi/180)\n",
    "\n",
    "def calculate_distance(pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude):\n",
    "    \n",
    "    from_lat = degree_to_radion(pickup_latitude)\n",
    "    from_long = degree_to_radion(pickup_longitude)\n",
    "    to_lat = degree_to_radion(dropoff_latitude)\n",
    "    to_long = degree_to_radion(dropoff_longitude)\n",
    "    \n",
    "    radius = 6371.01\n",
    "    \n",
    "    lat_diff = to_lat - from_lat\n",
    "    long_diff = to_long - from_long\n",
    "\n",
    "    a = np.sin(lat_diff / 2)**2 + np.cos(degree_to_radion(from_lat)) * np.cos(degree_to_radion(to_lat)) * np.sin(long_diff / 2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    \n",
    "    return radius * c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['distance'] = calculate_distance(df.start_lat, df.start_lon, df.end_lat, df.end_lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_date_time_features(dataset):\n",
    "    dataset['hour'] = dataset.start_time.dt.hour\n",
    "    dataset['day'] = dataset.start_time.dt.day\n",
    "    dataset['month'] = dataset.start_time.dt.month\n",
    "    dataset['year'] = dataset.start_time.dt.year\n",
    "    dataset['day_of_week'] = dataset.start_time.dt.dayofweek\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "df = add_new_date_time_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = convert_data_type(df,['start_station_complete','end_station_complete','bike_id'],float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['passholder_type','start_time','end_time','end_lat','end_lon','start_lat','start_lon']\n",
    "df = drop_column(df,drop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can use drop column function..\n",
    "# df.drop(columns=['passholder_type'],inplace=True)\n",
    "# df.drop(columns=['start_time','end_time'],inplace=True)\n",
    "# df.drop(columns=['end_lat','end_lon','start_lat','start_lon'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.start_station_complete = df.start_station_complete.astype(float)\n",
    "# df.end_station_complete = df.end_station_complete.astype(float)\n",
    "# df.bike_id = df.bike_id.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lat_null = df.loc[(df.end_lat.isnull()) & (df.start_lat.isnull())].index\n",
    "# df.drop(lat_null,inplace=True)\n",
    "# df.loc[(~df.end_lat.isnull()) & (df.start_lat.isnull())]\n",
    "######\n",
    "# df.loc[df['start_station_complete']=='3000.0']\n",
    "# df = df.loc[~df['start_lat'].isnull()]\n",
    "# df.loc[df['end_lat'].isnull()]['end_station_complete'].value_counts()\n",
    "# df.loc[df['end_station_complete'] == '90018.0']\n",
    "# df = df.loc[~df['end_lat'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.loc[(df['end_lat']!=0) | (df['end_lon']!=0)]\n",
    "# df = df.loc[(df['start_lat']!=0) | (df['start_lon']!=0)]\n",
    "# lat_special_char = df[df['start_lat'].str.contains('[A-Za-z]', na=False)].index\n",
    "# df.drop(lat_special_char,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['start_lat'] = df['start_lat'].astype(float)\n",
    "# df['start_lon'] = df['start_lon'].astype(float)\n",
    "# df['end_lat'] = df['end_lat'].astype(float)\n",
    "# df['end_lon'] = df['end_lon'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc[df.start_lat <=0,'start_lat'] = abs(df.start_lat)\n",
    "# df.loc[df.end_lat <=0,'end_lat'] = abs(df.end_lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lon_special_character = df[df['end_lat'].str.contains('[A-Za-z]', na=False)].index\n",
    "# df.drop(lon_special_character,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop(columns=['start_time','end_time'],inplace=True)\n",
    "# df.drop(columns=['end_lat','end_lon','start_lat','start_lon'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.start_station_complete = df.start_station_complete.astype(float)\n",
    "# df.end_station_complete = df.end_station_complete.astype(float)\n",
    "# df.bike_id = df.bike_id.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = LabelEncoder()\n",
    "df['trip_route'] = label.fit_transform(df['trip_route_category'])\n",
    "df.drop(columns='trip_route_category',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['duration'] = df['duration']/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['duration'])\n",
    "y = df['duration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "X = Normalizer().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.skew()\n",
    "y = np.log1p(y)\n",
    "sns.distplot(y, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score,mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dt = DecisionTreeRegressor()\n",
    "dt.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(np.sqrt(mean_squared_error(np.expm1(y),np.expm1(dt.predict(X)))))\n",
    "#lin_reg.predict(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
