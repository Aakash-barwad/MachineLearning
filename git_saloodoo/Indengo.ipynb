{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indego Bike Duration Prediction Using Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of content:\n",
    "\n",
    "1. loading all required packages.\n",
    "2. Loading data and converting into master file.\n",
    "3. Creation of preprocessing class ( Indengo ) which contain all operation in functional way.\n",
    "4. Creation of custom label encoder class for label encoding.\n",
    "5. Creattion of pipeline, train-test split, modelling and prediction.\n",
    "6. Saving and loading model into serializable object ( pickle )\n",
    "7. Last part some intial code from where i have arrived into some decisions - commented for now.\n",
    "\n",
    "**This is just rough notebook for operation which shows my steps and opearion, I have deployed model using Flask app so please refer flask app code for deployment, I have made powerBI dashboard for certain insight as well so please check that as well** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very First step ( optional ) , as data was not in proper nomenclature, so convert it into same nomenclature and collate in a single folder for easier access. In our case, 'data' folder contain all the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Importing important packages required for operation..\n",
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.base import TransformerMixin,BaseEstimator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder,Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all data files from the specific folder path or could be from remote machine path. Glob function is used to get all files list from directory, assuming all files is in .csv format for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data load function is used for loading data at a given directory or path.\n",
    "\n",
    "@param :\n",
    "       path : path could be directory of local or remote folder.\n",
    "       \n",
    "@return :\n",
    "      \n",
    "      df - data frame concatenated of all the files together in row wise ( axis = 0 )\n",
    "\n",
    "'''\n",
    "def data_load(path):\n",
    "    \n",
    "    files = glob.glob(\"%s\\*.csv\"%(path))\n",
    "    \n",
    "    for i,file in enumerate(files):\n",
    "        if i == 0 :\n",
    "            df = pd.read_csv(file)\n",
    "        else:\n",
    "            df = pd.concat([df,pd.read_csv(file)],axis=0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "#load data....\n",
    "df = data_load('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('whole_data.csv')\n",
    "df_copy = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A preprocessing class inherited transformer and estimator which basically gives flexibility to act as a transformer or estimator class when required.\n",
    "\n",
    "class contain fit method for now skipped ( later we can use to implement logical part )\n",
    "class contain transformer method which internally contained various method for preprocessing part ( will explain each method function and working at down )\n",
    "\n",
    "lastly this class return a cleaned dataframe which is used for modelling part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Indego(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit( self, df, y = None ):\n",
    "        print(\"in indego fit\")\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df, y = None):\n",
    "    \n",
    "        def time_convert(df,time_col):\n",
    "            df[time_col] = pd.to_datetime(arg=df[time_col], infer_datetime_format=True)\n",
    "            return df\n",
    "    \n",
    "        def drop_column(df, drop_col_name):\n",
    "            df.drop(columns=[drop_col_name],axis=1,inplace=True)\n",
    "            return df\n",
    "        \n",
    "        def punctuation_cleaning(df,punc_col):\n",
    "            char_index = df[df[punc_col].str.contains('[A-Za-z]', na=False)].index\n",
    "            df.drop(char_index,inplace=True)\n",
    "            return df\n",
    "        \n",
    "        def remove_null(df,col_name):\n",
    "            df = df.loc[~df[col_name].isnull()]\n",
    "            return df\n",
    "        \n",
    "        def convert_data_type(df, col_list,data_type):\n",
    "            df[col_list] = df[col_list].astype(data_type)\n",
    "            return df\n",
    "    \n",
    "        def station_cleaning(df, station_col_list,final_station_col):\n",
    "            drop_index = df.loc[(df[station_col_list[0]].isnull()) & (df[station_col_list[1]].isnull())].index\n",
    "            df.drop(drop_index,inplace=True)\n",
    "            df[station_col_list] = df[station_col_list].fillna('')\n",
    "            df[final_station_col] = df[station_col_list[0]].astype(str) +df[station_col_list[1]].astype(str)\n",
    "            df.drop(columns=station_col_list,inplace=True)\n",
    "            return df\n",
    "    \n",
    "        def lat_lon_cleaning(df,lat_lon_col):\n",
    "            lat_lon_null = df.loc[(df[lat_lon_col[0]].isnull()) & (df[lat_lon_col[1]].isnull())].index\n",
    "            df.drop(lat_lon_null,inplace=True)\n",
    "            df =  remove_null(df,lat_lon_col[1])\n",
    "            df = remove_null(df,lat_lon_col[0])\n",
    "            return df\n",
    "    \n",
    "        def remove_lat_lon_outlier(df,lat_lon_list):\n",
    "            df = df.loc[(df[lat_lon_list[0]]!=0) | (df[lat_lon_list[1]]!=0)]\n",
    "            df = punctuation_cleaning(df,lat_lon_list[0])\n",
    "            return df\n",
    "    \n",
    "        def change_lang_lat_value(df):\n",
    "            df.loc[df.start_lat <=0,'start_lat'] = abs(df.start_lat)\n",
    "            df.loc[df.end_lat <=0,'end_lat'] = abs(df.end_lat)\n",
    "            return df\n",
    "    \n",
    "        def degree_to_radion(degree):\n",
    "            return degree*(np.pi/180)\n",
    "\n",
    "        def calculate_distance(pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude):\n",
    "\n",
    "            from_lat = degree_to_radion(pickup_latitude)\n",
    "            from_long = degree_to_radion(pickup_longitude)\n",
    "            to_lat = degree_to_radion(dropoff_latitude)\n",
    "            to_long = degree_to_radion(dropoff_longitude)\n",
    "\n",
    "            radius = 6371.01\n",
    "\n",
    "            lat_diff = to_lat - from_lat\n",
    "            long_diff = to_long - from_long\n",
    "\n",
    "            a = np.sin(lat_diff / 2)**2 + np.cos(degree_to_radion(from_lat)) * np.cos(degree_to_radion(to_lat)) * np.sin(long_diff / 2)**2\n",
    "            c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "\n",
    "            return radius * c\n",
    "    \n",
    "        def add_new_date_time_features(dataset):\n",
    "            dataset['hour'] = dataset.start_time.dt.hour\n",
    "            dataset['day'] = dataset.start_time.dt.day\n",
    "            dataset['month'] = dataset.start_time.dt.month\n",
    "            dataset['year'] = dataset.start_time.dt.year\n",
    "            dataset['day_of_week'] = dataset.start_time.dt.dayofweek\n",
    "            dataset['quarter'] = dataset.start_time.dt.quarter\n",
    "\n",
    "            return dataset\n",
    "        \n",
    "        def add_temp_variable(df):\n",
    "            # Aug -nov : summer\n",
    "            # may - june -july : rainy \n",
    "            # dec - april - winter \n",
    "            df['temprature'] = 0\n",
    "            df.loc[(df['month']>=1) & (df['month']<=4),'temprature'] = 1\n",
    "            df.loc[(df['month']>=5) & (df['month']<=8),'temprature'] = 2\n",
    "            df.loc[(df['month']>=9) & (df['month']<=12),'temprature'] = 3\n",
    "            return df\n",
    "       \n",
    "        print(\"in Indengo transformer method\")\n",
    "        df = time_convert(df,'start_time')\n",
    "        df.set_index('trip_id',inplace=True)\n",
    "        df = station_cleaning(df,['start_station','start_station_id'],'start_station_complete')\n",
    "        df = station_cleaning(df,['end_station','end_station_id'],'end_station_complete')\n",
    "        df = punctuation_cleaning(df,'bike_id')\n",
    "        df = remove_null(df,'bike_id')\n",
    "        df = lat_lon_cleaning(df,['end_lat','start_lat'])\n",
    "        df = remove_lat_lon_outlier(df,['end_lat','end_lon'])\n",
    "        df = remove_lat_lon_outlier(df,['start_lat','start_lon'])\n",
    "        df = convert_data_type(df,['start_lat','start_lon','end_lat','end_lon'],float)\n",
    "        df = change_lang_lat_value(df)\n",
    "        df['distance'] = calculate_distance(df.start_lat, df.start_lon, df.end_lat, df.end_lon)\n",
    "        df = add_new_date_time_features(df)\n",
    "        df = add_temp_variable(df)\n",
    "        df = convert_data_type(df,['start_station_complete','end_station_complete','bike_id'],float)\n",
    "        drop_list = ['bike_type','passholder_type','start_time','end_time','end_lat','end_lon','start_lat','start_lon']\n",
    "        df.drop(columns = drop_list,axis=1,inplace=True)        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Custom label encoder is required as label encoder in sklearn can act in 1 column at a time and when crossing in pipeline we dont have flexibility to specify particular column which encoding is required.\n",
    "\n",
    "Custom label encoder also conatin estimator and transformer which will required during pipeline operation as pipeline will only accept estimator and transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_label_encoder(BaseEstimator,TransformerMixin):\n",
    "    def fit( self, df, y = None ):\n",
    "        print(\"in label encoder fit.....\")\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df, y = None):\n",
    "        print(\"in label encoder transformer\")\n",
    "        label = LabelEncoder()\n",
    "        df['trip_route'] = label.fit_transform(df['trip_route_category'])\n",
    "        df.drop(columns='trip_route_category',inplace=True)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we are done with function we need to create a model, first this should go through a pipeline. A pipeline is basically a sequence of steps arranged automatically.\n",
    "\n",
    "In our case for demo purpose i m breaking into 2 pipeline as we don't have external **test dataset** but we could have single pipeline as well.\n",
    "\n",
    "Here first divided into data preprocessing pipeline which is basically my main class ( **Indengo ) and custom label encode**.\n",
    "because these pipeline will preprocess data and gives a clean modelling data.\n",
    "\n",
    "Later on we have separated X and y variable, used train/test split (80-20%) for now, then as we have to predict in **minutes** and duration has given in seconds need to convert into minutes by dividing 60.\n",
    "\n",
    "We have notices that our **Target column** is skewed which need to be transformed so using log transformation here.\n",
    "\n",
    "once done we use our second pipeline, which is modelling pipeline and convert data into normalized first ( to scale it in 1 scale) so it should not crate any dominance.\n",
    "\n",
    "Then try with Model ( I had tried with (linear regression) baseline model) but i couldn't achieve and from there it clear this is not linear data. so directly trying with tree based model.\n",
    "\n",
    "Note : here we could check different model and compare best model with their metrics, for keeping it simpler using just decision tree. one more thing i have not performed hyperparameter tunning for now but we can do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_build():\n",
    "    preprocess_pipeline = Pipeline(steps=[('preprocess',Indego()),('label',custom_label_encoder())])\n",
    "    dataset = preprocess_pipeline.fit_transform(df) \n",
    "                                   \n",
    "    X = dataset.drop(columns=['duration'])\n",
    "    y = dataset['duration']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.20,random_state=42)\n",
    "    \n",
    "    y_train = y_train /60\n",
    "    y_test =  y_test / 60\n",
    "    \n",
    "    y_train = np.log1p(y_train)\n",
    "    y_test = np.log1p(y_test)\n",
    "    model = Pipeline(steps=[('Normalizer',Normalizer()),\n",
    "                               ('decision tree', DecisionTreeRegressor())\n",
    "                            ])\n",
    "    model.fit(X_train,y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    #print(np.expm1(model.predict(X_test)))\n",
    "    print(np.sqrt(mean_squared_error(np.expm1(y_test),np.expm1(model.predict(X_test)))))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once modelling is done next step to store serialize model ( pickle or any serializable format) . Here using pickling to store model and then saving as finalized_model.pkl.\n",
    "\n",
    "This is done because when we deploy and hit rest api ex : www.web.com/predict it will load model and perform predicton and return response.\n",
    "\n",
    "**can check flask app code where i have performed it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in indego fit\n",
      "in transformer method\n",
      "in label encoder fit.....\n",
      "in label encoder transformer\n",
      "58.631065229797066\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "model = model_build()\n",
    "filename = 'finalized_model.pkl'\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Wise explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding temprature variable in dataset because temprature is also a important parameter to ride or not. when checked in internet basically summer season is on ( aug-nov) and (may-june-july) is rainy season, other as winters. \n",
    "for time being divided in 4 months and assigned as 1,2,3 which is respectively winter, rainy and summer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Add temprature column in dataframe.\n",
    "\n",
    "@param :\n",
    "     df - a dataframe contained month column.\n",
    "\n",
    "@return :\n",
    "     df - a dataframe with temprature column along with other column.\n",
    "'''\n",
    "\n",
    "def add_temp_variable(df):\n",
    "    # Aug -nov : summer\n",
    "    # may - june -july : rainy \n",
    "    # dec - april - winter \n",
    "    df['temprature'] = 0\n",
    "    df.loc[(df['month']>=1) & (df['month']<=4),'temprature'] = 1\n",
    "    df.loc[(df['month']>=5) & (df['month']<=8),'temprature'] = 2\n",
    "    df.loc[(df['month']>=9) & (df['month']<=12),'temprature'] = 3\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Punctation cleaning is required because dataset metadata shows its integer column and as well approx 99% value are float/int only. secondly few rows have value like 'delete me' , it could be anything in future so for safer side remove all alpha value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Punctuation cleaning is used to clean any character present in integer or float column for now removing alphabets.\n",
    "\n",
    "@param :\n",
    "      df : a dataframe contain all the columns.\n",
    "      punc_col : a column contain special or alphabet character.\n",
    "      \n",
    "@return :\n",
    "      \n",
    "      df : a dataframe with removed puctutation from specific column.\n",
    "      \n",
    "'''\n",
    "def punctuation_cleaning(df,punc_col):\n",
    "    char_index = df[df[punc_col].str.contains('[A-Za-z]', na=False)].index\n",
    "    df.drop(char_index,inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convrsion of datetime column ( start_time ) to datetime datatype because it has string type, converting in datetime because we need to use property of date and time like day, month, year, quarter etc. \n",
    "\n",
    "This function is used only for **start_time** and not touching **end_time** because we won't have end_time at test data, i mean start_time is look ahead variable and end_time is your outcome variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Convert string type column into datetime format so we can use further its attribute.\n",
    "\n",
    "@param:\n",
    "    time_col : name of column which contain datetime value.\n",
    "\n",
    "@return:\n",
    "    df : dataframe which contain date time column.\n",
    "\n",
    "'''\n",
    "def time_convert(df,time_col):\n",
    "    df[time_col] = pd.to_datetime(arg=df[time_col], infer_datetime_format=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop a column from data frame which is not required or correlated with some other column. we need to create a simpler function at the end so we need to choose as minimal and effective column at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Drop a column from dataframe and return new dataframe without deleted column.\n",
    "\n",
    "@param:\n",
    "    df : dataframe contain all column including which need to be deleted.\n",
    "    drop_col_name : name of the column which need to be dropped.\n",
    "    \n",
    "@return:\n",
    "\n",
    "    df : a dataframe all column except deleted column.\n",
    "    \n",
    "'''\n",
    "def drop_column(df, drop_col_name):\n",
    "    df.drop(columns=[drop_col_name],inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we station column, first we see how much null values in each columns ( taking start_station & start_station_id ). I figured out both have contradictory in term of data. i.e. if station has value than station_id doesn't have and vice versa. that means this column correspond to same value so try to reduce in 1 column.\n",
    "\n",
    "first thing to remove those rows in which both of these column contain null values.\n",
    "later we fill blanks because we need to concatenate togther this column to get a complete new column without change of meaning.\n",
    "\n",
    "converted into str type to match type and added together and finally achieved a complete column, remove these two column as our new column is decribing two columns.\n",
    "\n",
    "same operation for end station as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Station cleaning is used to clean station and station_id.\n",
    "\n",
    "@param :\n",
    "      df : a dataframe contain all the colunmns\n",
    "      station_col_list: It contain start_station and start_station_id similarly for end station as well.\n",
    "      \n",
    "@return :\n",
    "      df : return a dataframe\n",
    "\n",
    "'''\n",
    "def station_cleaning(df, station_col_list,final_station_col):\n",
    "    drop_index = df.loc[(df[station_col_list[0]].isnull()) & (df[station_col_list[1]].isnull())].index\n",
    "    df.drop(drop_index,inplace=True)\n",
    "    df[station_col_list] = df[station_col_list].fillna('')\n",
    "    df[final_station_col] = df[station_col_list[0]].astype(str) +df[station_col_list[1]].astype(str)\n",
    "    df.drop(columns=station_col_list,inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to remove null which is beyond imputation, as i haven't been gone to business level of this usecase initial judgement of imputing would be wrong, however someplace dropping null values is good because they will unwantedely creates anamoly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Remove null values from a specific column.\n",
    "\n",
    "@param :\n",
    "        df - a dataframe which contain all columns\n",
    "        col_name - name of column which contain null values.\n",
    "        \n",
    "@return :\n",
    "       df - a dataframe with removed null values from specific column.\n",
    "\n",
    "'''\n",
    "\n",
    "def remove_null(df,col_name):\n",
    "    df = df.loc[~df[col_name].isnull()]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latitude and longitude cleaning is similar to station cleaning, first we need to check if both start and end have 0 then we have to remove those rows.\n",
    "\n",
    "And as we are calculating distance at later stage we have to remove any null value in both start and end latitude as it will correspond to 0 distance which will be anamoly.\n",
    "\n",
    "I had tried to impute these but when you see for those null value station id = 3000 which is a virtual station, so whereever virtualstation present there is no lat and longitude which is ok also. for now remove this rows but we can have impute based on some business logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "cleaning latitude and longitude which is enable for both ( start and end ). \n",
    "\n",
    "@param :\n",
    "        df - a dataframe which contain all columns\n",
    "        lat_lon_col - list of latitude and longitude column ex [start_lat, start_lon].\n",
    "        \n",
    "@return :\n",
    "       df - a dataframe with clean values from lat, long column.\n",
    "\n",
    "'''\n",
    "def lat_lon_cleaning(df,lat_lon_col):\n",
    "    lat_lon_null = df.loc[(df[lat_lon_col[0]].isnull()) & (df[lat_lon_col[1]].isnull())].index\n",
    "    df.drop(lat_lon_null,inplace=True)\n",
    "    df = remove_null(df,lat_lon_col[1])\n",
    "    df = remove_null(df,lat_lon_col[0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In latitude and longitude column , noticed some 0 value rows which is anamoly because our area of region lies in ( 39 , -75). secondly some of string value also mentioned in the some rows \"//n\" which need to be removed. This operation used for both start and end field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "cleaning latitude and longitude which is enable for both ( start and end ) which have 0 values as well as some \n",
    "non lat,long values like here it is \"\\\\n\" which should be removed. \n",
    "\n",
    "@param :\n",
    "        df - a dataframe which contain all columns\n",
    "        lat_lon_col - list of latitude and longitude column ex [start_lat, start_lon].\n",
    "        \n",
    "@return :\n",
    "       df - a dataframe with clean values from lat, long column.\n",
    "'''\n",
    "\n",
    "def remove_lat_lon_outlier(df,lat_lon_list):\n",
    "    df = df.loc[(df[lat_lon_list[0]]!=0) | (df[lat_lon_list[1]]!=0)]\n",
    "    df = punctuation_cleaning(df,lat_lon_list[0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert data type from one data type to other data type, this is required at end because for modelling we required either float or integer value, but if we see some of our column , we found out string column which need to be converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "convert data type of column. \n",
    "\n",
    "@param :\n",
    "        df - a dataframe which contain all columns\n",
    "        col_list - list of column to be converted.\n",
    "        data_type - data type in which column need to be converted.\n",
    "        \n",
    "@return :\n",
    "       df - a dataframe with changed data type for specified field.\n",
    "'''\n",
    "\n",
    "def convert_data_type(df, col_list,data_type):\n",
    "    df[col_list] = df[col_list].astype(data_type)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In latitude column some of values by mistakely given as -ve which is actually a +ve so convert that neg into positive.\n",
    "\n",
    "Example we have a latitude approx 39.2 - 39.7 etc but some places bymistakenly they have done -39.2 that should be 39.2 which need to be corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "anamoly cleaning latitude which is enable for both ( start and end ). \n",
    "\n",
    "@param :\n",
    "        df - a dataframe which contain all columns\n",
    "        \n",
    "@return :\n",
    "       df - a dataframe with clean values from lat column.\n",
    "\n",
    "'''\n",
    "\n",
    "def change_lang_lat_value(df):\n",
    "    df.loc[df.start_lat <=0,'start_lat'] = abs(df.start_lat)\n",
    "    df.loc[df.end_lat <=0,'end_lat'] = abs(df.end_lat)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generating distance attribute based on start and end (latitude,longitude). here basically converting longitude and latitude in radian and then using manhatten metrics to convert into distance.\n",
    "\n",
    "converting into distance will give a huge bump up because distance is entirely related to duration to reach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Generating distance using manhatten distance formula using start and end latitude and longitude point. \n",
    "\n",
    "@param :\n",
    "        df - a dataframe which contain all columns\n",
    "        pickup_latitude - start latitude\n",
    "        pickup_longitude - start longitude\n",
    "        dropoff_latitude - end latitude\n",
    "        dropoff_longitude - end longitude\n",
    "        \n",
    "@return :\n",
    "       distance -  calculated distance.\n",
    "'''\n",
    "\n",
    "def degree_to_radion(degree):\n",
    "    return degree*(np.pi/180)\n",
    "\n",
    "def calculate_distance(pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude):\n",
    "    \n",
    "    from_lat = degree_to_radion(pickup_latitude)\n",
    "    from_long = degree_to_radion(pickup_longitude)\n",
    "    to_lat = degree_to_radion(dropoff_latitude)\n",
    "    to_long = degree_to_radion(dropoff_longitude)\n",
    "    \n",
    "    radius = 6371.01\n",
    "    \n",
    "    lat_diff = to_lat - from_lat\n",
    "    long_diff = to_long - from_long\n",
    "\n",
    "    a = np.sin(lat_diff / 2)**2 + np.cos(degree_to_radion(from_lat)) * np.cos(degree_to_radion(to_lat)) * np.sin(long_diff / 2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    \n",
    "    return radius * c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting datetime column into different attribute of it like day,month,year,dayofweek,quarter etc. each variable has its significant like hour basis we can check whether what time of day, ride frequency increases/decreases.\n",
    "converting into quarters which lead you to convert into temprature as quarterly we are specifing temprature. \n",
    "dayofweek and day wise we can check whether holiday or not, how much ride has been take off in weekend etc. info we can achieve from this attributes so improtant to unflod this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Generating datetime attribute using date time column. \n",
    "\n",
    "@param :\n",
    "        df - a dataframe which contain all columns\n",
    "        \n",
    "@return :\n",
    "       df -  dataframe with hour,day,month,year etc.\n",
    "'''\n",
    "\n",
    "def add_new_date_time_features(dataset):\n",
    "    dataset['hour'] = dataset.start_time.dt.hour\n",
    "    dataset['day'] = dataset.start_time.dt.day\n",
    "    dataset['month'] = dataset.start_time.dt.month\n",
    "    dataset['year'] = dataset.start_time.dt.year\n",
    "    dataset['day_of_week'] = dataset.start_time.dt.dayofweek\n",
    "    dataset['quarter'] = dataset.start_time.dt.quarter\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Intial work and Rough work - commented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_pipeline = Pipeline(steps=[('preprocess',Indego())])\n",
    "# temp = pre_pipeline.fit_transform(df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = add_temp_variable(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp.month.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# station = temp.start_station_complete.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp.day_of_week.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp.groupby('hour').count()['distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.barplot(df['hour'],df['duration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.barplot(df['hour'],df['distance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BB = (-75.5, -76.8, 39.5, 40.8)\n",
    "# nyc_map = plt.imread('phiadelphia.PNG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will be used more often to plot data on the NYC map\n",
    "# df.groupby('start_lat').count()\n",
    "\n",
    "# def plot_on_map(df, BB, nyc_map, s=10, alpha=0.2):\n",
    "#     fig, axs = plt.subplots(1, 2, figsize=(16,10))\n",
    "#     axs[0].scatter(df.start_lon, df.start_lat, zorder=1, alpha=alpha, c='r', s=s)\n",
    "#     axs[0].set_xlim((BB[0], BB[1]))\n",
    "#     axs[0].set_ylim((BB[2], BB[3]))\n",
    "#     axs[0].set_title('Pickup locations')\n",
    "#     axs[0].imshow(nyc_map, zorder=0, extent=BB)\n",
    "\n",
    "# plot_on_map(df, BB, nyc_map, s=1, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = punctuation_cleaning(df,'start_lat')\n",
    "# df = punctuation_cleaning(df,'end_lat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set trip id as index which is unique as well as complete.\n",
    "# df.set_index('trip_id',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorting based on start time so we can check time wise trend and changes..\n",
    "# df.sort_values('start_time',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop bike type as almost more than 50% record are null and other 50% have biased towards specific value.. \n",
    "# df.drop(columns=['bike_type'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = station_cleaning(df,['start_station','start_station_id'],'start_station_complete')\n",
    "# df = station_cleaning(df,['end_station','end_station_id'],'end_station_complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_index = df.loc[(df.start_station.isnull()) & (df.start_station_id.isnull())].index\n",
    "# df.drop(drop_index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc[(~df.start_station.isnull()) & (df.start_station_id.isnull())].shape\n",
    "# df.loc[(df.start_station.isnull()) & (~df.start_station_id.isnull())].shape\n",
    "# df[['start_station','start_station_id']] = df[['start_station','start_station_id']].fillna('')\n",
    "# df['start_station_complete'] = df['start_station'].astype(str) +df['start_station_id'].astype(str)\n",
    "# df.drop(columns=['start_station','start_station_id'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc[(~df.end_station.isnull()) & (df.end_station_id.isnull())].shape\n",
    "# df.loc[(df.end_station.isnull()) & (~df.end_station_id.isnull())].shape\n",
    "# df[['end_station','end_station_id']] = df[['end_station','end_station_id']].fillna('')\n",
    "# df['end_station_complete'] = df['end_station'].astype(str) +df['end_station_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop(columns=['end_station','end_station_id'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df['bike_id'].str.contains('[A-Za-z]',na=False)]\n",
    "# df = df.loc[df['bike_id'] !='delete me']\n",
    "# char_index = df[df['bike_id'].str.contains('[A-Za-z]', na=False)].index\n",
    "# df.drop(char_index,inplace=True)\n",
    "# df = df.loc[~df.bike_id.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = punctuation_cleaning(df,'bike_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = remove_null(df,'bike_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = remove_lat_lon_outlier(df,['end_lat','end_lon'])\n",
    "# df = remove_lat_lon_outlier(df,['start_lat','start_lon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = convert_data_type(df,['start_lat','start_lon','end_lat','end_lon'],float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = change_lang_lat_value(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['distance'] = calculate_distance(df.start_lat, df.start_lon, df.end_lat, df.end_lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = convert_data_type(df,['start_station_complete','end_station_complete','bike_id'],float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_list = ['passholder_type','start_time','end_time','end_lat','end_lon','start_lat','start_lon']\n",
    "# df = drop_column(df,drop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can use drop column function..\n",
    "# df.drop(columns=['passholder_type'],inplace=True)\n",
    "# df.drop(columns=['start_time','end_time'],inplace=True)\n",
    "# df.drop(columns=['end_lat','end_lon','start_lat','start_lon'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.start_station_complete = df.start_station_complete.astype(float)\n",
    "# df.end_station_complete = df.end_station_complete.astype(float)\n",
    "# df.bike_id = df.bike_id.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lat_null = df.loc[(df.end_lat.isnull()) & (df.start_lat.isnull())].index\n",
    "# df.drop(lat_null,inplace=True)\n",
    "# df.loc[(~df.end_lat.isnull()) & (df.start_lat.isnull())]\n",
    "######\n",
    "# df.loc[df['start_station_complete']=='3000.0']\n",
    "# df = df.loc[~df['start_lat'].isnull()]\n",
    "# df.loc[df['end_lat'].isnull()]['end_station_complete'].value_counts()\n",
    "# df.loc[df['end_station_complete'] == '90018.0']\n",
    "# df = df.loc[~df['end_lat'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.loc[(df['end_lat']!=0) | (df['end_lon']!=0)]\n",
    "# df = df.loc[(df['start_lat']!=0) | (df['start_lon']!=0)]\n",
    "# lat_special_char = df[df['start_lat'].str.contains('[A-Za-z]', na=False)].index\n",
    "# df.drop(lat_special_char,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['start_lat'] = df['start_lat'].astype(float)\n",
    "# df['start_lon'] = df['start_lon'].astype(float)\n",
    "# df['end_lat'] = df['end_lat'].astype(float)\n",
    "# df['end_lon'] = df['end_lon'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc[df.start_lat <=0,'start_lat'] = abs(df.start_lat)\n",
    "# df.loc[df.end_lat <=0,'end_lat'] = abs(df.end_lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lon_special_character = df[df['end_lat'].str.contains('[A-Za-z]', na=False)].index\n",
    "# df.drop(lon_special_character,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop(columns=['start_time','end_time'],inplace=True)\n",
    "# df.drop(columns=['end_lat','end_lon','start_lat','start_lon'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.start_station_complete = df.start_station_complete.astype(float)\n",
    "# df.end_station_complete = df.end_station_complete.astype(float)\n",
    "# df.bike_id = df.bike_id.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['duration'] = df['duration']/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df.drop(columns=['duration'])\n",
    "# y = df['duration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y.skew()\n",
    "# y = np.log1p(y)\n",
    "# sns.distplot(y, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.metrics import r2_score,mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# dt = DecisionTreeRegressor()\n",
    "# dt.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lin_reg = LinearRegression()\n",
    "# lin_reg.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.sqrt(mean_squared_error(np.expm1(y),np.expm1(dt.predict(X)))))\n",
    "#lin_reg.predict(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
